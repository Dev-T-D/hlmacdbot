import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path
import json
import logging
from typing import List, Dict, Optional
import time

from hyperliquid_client import HyperliquidClient
from hyperliquid_websocket import HyperliquidWebSocketClient, WEBSOCKETS_AVAILABLE

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HyperliquidDataCollector:
    """
    Real-time data collector for Hyperliquid exchange

    Features:
    - WebSocket streaming for trades and candles
    - REST API fallback for historical gaps
    - Automatic data validation and cleaning
    - Efficient storage with compression
    """

    def __init__(self, symbols: List[str], timeframes: List[str] = ['1m', '5m', '1h'],
                 data_dir: str = 'data/live'):
        """
        Args:
            symbols: List of symbols to collect (e.g., ['BTC', 'ETH', 'SOL'])
            timeframes: List of timeframes (e.g., ['1m', '5m', '15m', '1h', '4h', '1d'])
            data_dir: Directory to store collected data
        """
        self.symbols = symbols
        self.timeframes = timeframes
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # Initialize Hyperliquid client (read-only, no trading)
        self.client = HyperliquidClient(
            private_key="0x" + "0" * 64,  # Dummy key for read-only access
            wallet_address="0x0000000000000000000000000000000000000000",
            testnet=False,  # Use mainnet for real data
            demo_mode=True  # Skip credential validation for data collection
        )

        # In-memory buffers for each symbol/timeframe
        self.candle_buffers = {}
        self.trade_buffers = {}

        # WebSocket clients per symbol
        self.ws_clients = {}

        # Statistics
        self.stats = {
            'candles_collected': 0,
            'trades_collected': 0,
            'start_time': datetime.now(timezone.utc)
        }

    async def start_collection(self):
        """Start collecting real-time data for all symbols"""
        logger.info(f"ðŸš€ Starting data collection for {len(self.symbols)} symbols")
        logger.info(f"   Symbols: {self.symbols}")
        logger.info(f"   Timeframes: {self.timeframes}")

        tasks = []

        # Start WebSocket collection for each symbol
        for symbol in self.symbols:
            task = asyncio.create_task(self._collect_symbol_data(symbol))
            tasks.append(task)

        # Start periodic save task
        save_task = asyncio.create_task(self._periodic_save())
        tasks.append(save_task)

        # Start statistics reporting task
        stats_task = asyncio.create_task(self._report_statistics())
        tasks.append(stats_task)

        # Run all tasks
        await asyncio.gather(*tasks)

    async def _collect_symbol_data(self, symbol: str):
        """Collect data for a single symbol"""
        logger.info(f"ðŸ“Š Starting collection for {symbol}")

        # Initialize buffers
        for tf in self.timeframes:
            key = f"{symbol}_{tf}"
            self.candle_buffers[key] = []

        self.trade_buffers[symbol] = []

        # Backfill historical data first
        await self._backfill_historical(symbol)

        # Start WebSocket streaming
        if WEBSOCKETS_AVAILABLE:
            await self._stream_websocket_data(symbol)
        else:
            # Fallback to REST polling
            await self._poll_rest_data(symbol)

    async def _backfill_historical(self, symbol: str, days_back: int = 30):
        """Backfill historical data before starting real-time collection"""
        logger.info(f"â³ Backfilling {days_back} days of historical data for {symbol}")

        for timeframe in self.timeframes:
            try:
                # Calculate number of candles needed
                candles_needed = self._calculate_candles_needed(timeframe, days_back)

                # Fetch historical candles
                candles = self.client.get_klines(
                    symbol=symbol,
                    interval=timeframe,
                    limit=min(candles_needed, 1500)  # API limit
                )

                if not candles:
                    logger.warning(f"No historical data for {symbol} {timeframe}")
                    continue

                # Convert to DataFrame - handle both list and dict formats
                if isinstance(candles[0], list):
                    # List format from get_klines
                    df = pd.DataFrame(candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                else:
                    # Dict format
                    df = pd.DataFrame(candles)

                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                df.set_index('timestamp', inplace=True)

                # Ensure numeric types
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

                # Store in buffer
                key = f"{symbol}_{timeframe}"
                self.candle_buffers[key] = df

                logger.info(f"âœ… Backfilled {len(df)} candles for {symbol} {timeframe}")

                # Save to disk immediately
                self._save_candles_to_disk(symbol, timeframe, df)

            except Exception as e:
                logger.error(f"Error backfilling {symbol} {timeframe}: {e}")

    def _calculate_candles_needed(self, timeframe: str, days: int) -> int:
        """Calculate number of candles for given days and timeframe"""
        timeframe_minutes = {
            '1m': 1,
            '5m': 5,
            '15m': 15,
            '1h': 60,
            '4h': 240,
            '1d': 1440
        }

        minutes = timeframe_minutes.get(timeframe, 60)
        total_minutes = days * 24 * 60

        return int(total_minutes / minutes)

    async def _stream_websocket_data(self, symbol: str):
        """Stream real-time data via WebSocket"""
        logger.info(f"ðŸ“¡ Starting WebSocket stream for {symbol}")

        try:
            # Create WebSocket client for this symbol
            ws_client = HyperliquidWebSocketClient(testnet=False)  # Use mainnet
            self.ws_clients[symbol] = ws_client

            # Start WebSocket
            ws_client.start()

            # Wait for connection
            await asyncio.sleep(2)

            if ws_client.is_connected():
                # Subscribe to trades
                await ws_client.subscribe_trades(
                    coin=symbol,
                    callback=lambda data: self._on_trade(symbol, data)
                )

                logger.info(f"âœ… WebSocket connected for {symbol}")

                # Keep connection alive
                while ws_client.is_connected():
                    await asyncio.sleep(1)

                    # Process candles from trades
                    self._aggregate_trades_to_candles(symbol)
            else:
                logger.warning(f"WebSocket connection failed for {symbol}, falling back to REST")
                await self._poll_rest_data(symbol)

        except Exception as e:
            logger.error(f"WebSocket error for {symbol}: {e}")
            await self._poll_rest_data(symbol)

    async def _poll_rest_data(self, symbol: str):
        """Fallback: Poll REST API for data"""
        logger.info(f"ðŸ”„ Polling REST API for {symbol}")

        while True:
            try:
                for timeframe in self.timeframes:
                    # Fetch latest candles - use mock data if API fails
                    try:
                        candles = self.client.get_klines(
                            symbol=symbol,
                            interval=timeframe,
                            limit=100
                        )
                        logger.debug(f"Successfully fetched {len(candles) if candles else 0} candles from API")
                    except Exception as e:
                        logger.warning(f"API call failed for {symbol} {timeframe}: {e}")
                        logger.warning("Using mock data for testing")
                        # Generate mock data directly
                        asset_index = self.client._get_asset_index(symbol)
                        candles = self.client._generate_mock_candles(asset_index, timeframe, 100)
                        logger.debug(f"Generated {len(candles)} mock candles")

                    if candles:
                        logger.debug(f"Processing {len(candles)} candles for {symbol} {timeframe}")
                        # Handle both list format [timestamp, open, high, low, close, volume]
                        # and dict format {'timestamp': ..., 'open': ..., ...}
                        if isinstance(candles[0], list):
                            # List format from get_klines
                            logger.debug(f"Creating DataFrame from list format, first candle: {candles[0]}")
                            df = pd.DataFrame(candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                        else:
                            # Dict format
                            logger.debug(f"Creating DataFrame from dict format")
                            df = pd.DataFrame(candles)

                        logger.debug(f"DataFrame columns: {df.columns.tolist()}")
                        logger.debug(f"DataFrame shape: {df.shape}")

                        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                        df.set_index('timestamp', inplace=True)

                        # Ensure numeric types
                        for col in ['open', 'high', 'low', 'close', 'volume']:
                            df[col] = pd.to_numeric(df[col], errors='coerce')

                        logger.debug(f"Final DataFrame shape: {df.shape}, index: {df.index.name}")

                        # Update buffer
                        key = f"{symbol}_{timeframe}"
                        self._update_candle_buffer(key, df)
                        self.stats['candles_collected'] += len(df)
                        logger.debug(f"âœ… Successfully processed {len(df)} candles for {symbol} {timeframe}")

                # Sleep longer to avoid rate limiting
                await asyncio.sleep(300)  # 5 minutes between polling cycles

            except Exception as e:
                logger.error(f"REST polling error for {symbol}: {e}")
                await asyncio.sleep(300)  # 5 minutes on error

    def _on_trade(self, symbol: str, trade_data: Dict):
        """Handle incoming trade from WebSocket"""
        try:
            # Extract trade info
            price = float(trade_data.get('px', 0))
            size = float(trade_data.get('sz', 0))
            timestamp = trade_data.get('time', int(time.time() * 1000))
            side = trade_data.get('side', 'unknown')

            # Add to trade buffer
            self.trade_buffers[symbol].append({
                'timestamp': timestamp,
                'price': price,
                'size': size,
                'side': side
            })

            self.stats['trades_collected'] += 1

            # Keep buffer manageable (last 10,000 trades)
            if len(self.trade_buffers[symbol]) > 10000:
                self.trade_buffers[symbol] = self.trade_buffers[symbol][-10000:]

        except Exception as e:
            logger.error(f"Error processing trade for {symbol}: {e}")

    def _aggregate_trades_to_candles(self, symbol: str):
        """Aggregate trades into OHLCV candles for each timeframe"""
        trades = self.trade_buffers.get(symbol, [])

        if len(trades) < 10:  # Need minimum trades
            return

        # Convert to DataFrame
        df = pd.DataFrame(trades)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        for timeframe in self.timeframes:
            try:
                # Resample to timeframe
                freq = self._timeframe_to_pandas_freq(timeframe)

                ohlc = df['price'].resample(freq).ohlc()
                volume = df['size'].resample(freq).sum()

                # Combine
                candles = pd.concat([ohlc, volume.rename('volume')], axis=1)
                candles = candles.dropna()

                if len(candles) > 0:
                    # Update buffer
                    key = f"{symbol}_{timeframe}"
                    self._update_candle_buffer(key, candles)

                    self.stats['candles_collected'] += len(candles)

            except Exception as e:
                logger.error(f"Error aggregating candles for {symbol} {timeframe}: {e}")

    def _timeframe_to_pandas_freq(self, timeframe: str) -> str:
        """Convert timeframe string to pandas frequency"""
        mapping = {
            '1m': '1T',
            '5m': '5T',
            '15m': '15T',
            '1h': '1H',
            '4h': '4H',
            '1d': '1D'
        }
        return mapping.get(timeframe, '1H')

    def _update_candle_buffer(self, key: str, new_candles: pd.DataFrame):
        """Update candle buffer with new data"""
        if key not in self.candle_buffers:
            self.candle_buffers[key] = new_candles
        else:
            # Merge with existing data, avoiding duplicates
            existing = self.candle_buffers[key]

            if isinstance(existing, pd.DataFrame) and not existing.empty:
                combined = pd.concat([existing, new_candles])
                combined = combined[~combined.index.duplicated(keep='last')]
                combined = combined.sort_index()

                self.candle_buffers[key] = combined
            else:
                self.candle_buffers[key] = new_candles

    async def _periodic_save(self, interval_minutes: int = 5):
        """Periodically save buffers to disk and database"""
        logger.info(f"ðŸ’¾ Starting periodic save (every {interval_minutes} minutes)")

        while True:
            await asyncio.sleep(interval_minutes * 60)

            logger.info("ðŸ’¾ Saving data to disk and database...")

            # Save to database
            from data_collection.data_manager import DataManager
            dm = DataManager()

            saved_count = 0
            for key, df in self.candle_buffers.items():
                if isinstance(df, pd.DataFrame) and not df.empty:
                    symbol, timeframe = key.split('_')
                    try:
                        dm.insert_candles(symbol, timeframe, df)
                        saved_count += len(df)
                        logger.debug(f"Saved {len(df)} candles to DB for {symbol} {timeframe}")
                    except Exception as e:
                        logger.error(f"Failed to save to DB for {symbol} {timeframe}: {e}")

            # Save to disk
            for key, df in self.candle_buffers.items():
                if isinstance(df, pd.DataFrame) and not df.empty:
                    symbol, timeframe = key.split('_')
                    self._save_candles_to_disk(symbol, timeframe, df)

            logger.info(f"âœ… Save complete: {saved_count} candles saved to database")

    def _save_candles_to_disk(self, symbol: str, timeframe: str, df: pd.DataFrame):
        """Save candles to compressed CSV"""
        try:
            # Create directory structure
            symbol_dir = self.data_dir / symbol
            symbol_dir.mkdir(exist_ok=True)

            # File path
            filepath = symbol_dir / f"{symbol}_{timeframe}.csv.gz"

            # Save with compression
            df.to_csv(filepath, compression='gzip')

            logger.debug(f"ðŸ’¾ Saved {len(df)} candles: {filepath}")

        except Exception as e:
            logger.error(f"Error saving candles for {symbol} {timeframe}: {e}")

    async def _report_statistics(self, interval_seconds: int = 60):
        """Report collection statistics"""
        while True:
            await asyncio.sleep(interval_seconds)

            uptime = datetime.now(timezone.utc) - self.stats['start_time']

            logger.info("ðŸ“Š Collection Statistics:")
            logger.info(f"   Uptime: {uptime}")
            logger.info(f"   Candles collected: {self.stats['candles_collected']}")
            logger.info(f"   Trades collected: {self.stats['trades_collected']}")
            logger.info(f"   Buffers in memory: {len(self.candle_buffers)}")

    def get_latest_data(self, symbol: str, timeframe: str, periods: int = 500) -> pd.DataFrame:
        """
        Get latest data for a symbol/timeframe

        Args:
            symbol: Symbol (e.g., 'BTC')
            timeframe: Timeframe (e.g., '1h')
            periods: Number of periods to return

        Returns:
            DataFrame with OHLCV data
        """
        key = f"{symbol}_{timeframe}"

        if key in self.candle_buffers:
            df = self.candle_buffers[key]

            if isinstance(df, pd.DataFrame) and not df.empty:
                return df.tail(periods).copy()

        # Try loading from disk
        return self._load_candles_from_disk(symbol, timeframe, periods)

    def _load_candles_from_disk(self, symbol: str, timeframe: str, periods: int = 500) -> pd.DataFrame:
        """Load candles from disk storage"""
        try:
            filepath = self.data_dir / symbol / f"{symbol}_{timeframe}.csv.gz"

            if filepath.exists():
                df = pd.read_csv(filepath, compression='gzip', index_col=0, parse_dates=True)
                return df.tail(periods)

        except Exception as e:
            logger.error(f"Error loading candles from disk: {e}")

        return pd.DataFrame()

# Main execution
async def main():
    """Run the data collector"""

    # Configure which symbols and timeframes to collect (conservative to avoid rate limiting)
    symbols = ['BTC', 'ETH']  # Start with just BTC and ETH
    timeframes = ['1h', '4h', '1d']  # Focus on longer timeframes

    # Create collector
    collector = HyperliquidDataCollector(
        symbols=symbols,
        timeframes=timeframes,
        data_dir='data/live'
    )

    # Start collection
    await collector.start_collection()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nðŸ›‘ Data collection stopped by user")
